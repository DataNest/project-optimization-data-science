{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT: Course Optimization for Data Science\n",
    "## Optimization strategies for Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Alexandre Gramfort, Stéphane Gaiffas\n",
    "\n",
    "If you have questions or if something is not clear in the text below please contact us\n",
    "by email.\n",
    "\n",
    "## Aim:\n",
    "\n",
    "- derive the duals for SVMs with and without intercept\n",
    "- implement an SVM using a blackbox convex toolbox (cvxopt in Python)\n",
    "- implement your own solvers for the without intercept case: Proximal gradient, Coordinate Descent, Newton, Quasi-Newton\n",
    "- Present a clear benchmark of the different strategies on small and medium scale datasets\n",
    "\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "This work must be done by pairs of students.\n",
    "Each student must send their work before the 3rd of January at 23:59, using the moodle platform.\n",
    "This means that **each student in the pair sends the same file**\n",
    "\n",
    "On the moodle, in the \"Optimization for Data Science\" course, you have a \"devoir\" section called \"Project\".\n",
    "This is where you submit your jupyter notebook file.\n",
    "\n",
    "The name of the file must be constructed as in the next cell\n",
    "\n",
    "### Gentle reminder: no evaluation if you don't respect this EXACTLY\n",
    "\n",
    "#### How to construct the name of your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_el_alaoui_salma_and_hung_chia_man_and_liu_zhengying.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Change here using YOUR first and last names\n",
    "fn1 = \"salma\"\n",
    "ln1 = \"el_alaoui\"\n",
    "fn2 = \"chia_man\"\n",
    "ln2 = \"hung\"\n",
    "fn3 = \"zhengying\"\n",
    "ln3 = \"liu\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(), \n",
    "                        [\"project\", ln1, fn1, \"and\", ln2, fn2, \"and\", ln3, fn3])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important:\n",
    "\n",
    "For Part 0 to Part 2 of the project you will need a working install of `cvxopt`.\n",
    "You may struggle a bit to set it up.\n",
    "The simplest way of getting it is by typing \n",
    "\n",
    "`pip install cvxopt`\n",
    "\n",
    "if you have `pip` installed on your laptop.\n",
    "If you **struggle too much please\n",
    "contact us**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: SVM Classification with linear kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the problem of binary classification from $n$ observations\n",
    "$x_i \\in \\mathbb{R}^{d}$,\n",
    "$1 \\leq i \\leq n$. We aim to learn a function:\n",
    "$$f: x \\in \\mathbb{R}^{d}\\mapsto y\\in\\{-1,+1\\}$$\n",
    "from the $n$ annotated training samples $(x_{i},y_{i})$ supposed i.i.d. from an unknown probability distribution on $\\mathbb{R}^d \\times \\{-1,+1\\}$. Once this function is learnt, it will be possible to use it to predict the label $y$ associated to a new sample $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Margin and linear separating hyperplane:\n",
    "\n",
    "<img src=\"separateur.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the linear case, one looks for an affine function of $x$ of the form \n",
    "$f(x) = \\mathrm{sign}(w^{\\top} x)$ or $f(x)=\\mathrm{sign}(w^{\\top}x + b)$\n",
    "with $w \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$. The first case is referred\n",
    "to as the **without intercept** case. Indeed the coefficient $b$ is known\n",
    "as the intercept or bias term.\n",
    "\n",
    "We will start by considering the case with intercept.\n",
    "\n",
    "To learn $f$, we use the $n$ annotated samples and one looks for a hyperplane $P(w,b)$\n",
    "such that the smallest distance to positive and negative samples\n",
    "is the largest. This can be written as:\n",
    "$$\n",
    " \\max_{w,b} \\min_{i=1:n} y_i \\delta(x_{i},P(w,b)) \\quad\n",
    " \\text{where}\\quad \\delta(x_{i},w,b) = \\frac{w^{\\top}x_{i}+b}{\\sqrt{w^{\\top}w}} \\enspace,\n",
    "$$\n",
    "since the signed distance from a sample $x_{i}$ to the hyperplane $P(w,b)$ is given by\n",
    "$\n",
    "\\delta(x_{i},w,b)\n",
    "$.\n",
    "The principle described above is the maximisation of the *margin*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can notice that if the minimum of a set of values is larger than $m$ than all values of the set are larger than $m$. This leads to the following problem formulation:\n",
    "$$\n",
    " \\left\\{\n",
    " \\begin{array}{cll}\n",
    " \\max_{(w,b)} \\quad m \\\\\n",
    " \\text{s.t.} \\;\\; &\\forall i &y_i\\dfrac{w^{\\top}x_{i}+b}{\\sqrt{w^{\\top}w}}\\geq m\n",
    " \\end{array}\n",
    " \\right. \\enspace .\n",
    "$$\n",
    "\n",
    "The hyperplane separates the space in 2 half spaces, depending if $\\delta(x_{i},w,b)$ is positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming all samples are linearly separable, convince yourself that the problem can be written as:\n",
    "$$\n",
    "(\\mathcal{P}):  \\left\\{\n",
    " \\begin{array}{cll}\n",
    " &\\min_{(w,b)} \\frac{1}{2}w^{\\top}w\n",
    " \\\\\n",
    "  &y_{i}(w^{\\top}x_{i}+b)\\geq 1, \\quad \\forall i\\in \\{1,\\cdots,n\\}\n",
    " \\end{array}\n",
    " \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q1: Justify that the problem $(\\mathcal{P})$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A1: We can rewrite the problem as\n",
    "$$\n",
    "(\\mathcal{P}):  \\left\\{\n",
    " \\begin{array}{cll}\n",
    " &\\min_{(w,b)} J(w,b)\n",
    " \\\\\n",
    "  &F_i(w,b)\\leq 0, \\quad \\forall i\\in \\{1,\\cdots,n\\}\n",
    " \\end{array}\n",
    " \\right.\n",
    "$$\n",
    "where $J(w,b)=\\frac{1}{2}w^{\\top}w$ and $F_i(w,b)=1-y_{i}(w^{\\top}x_{i}+b)$. We see that $J(w)$ is quadratic and definite positive thus convex. $F_i(w)$ are affine and thus convex too.\n",
    "\n",
    "In conclusion, the problem $(\\mathcal{P})$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q2: By looking at the saddle points of the Lagrangian $\\mathcal{L}(w, b, \\mu)$, $\\mu \\in \\mathbb{R}_+^n$, show that the dual problem $(\\mathcal{D})$ can be written as:\n",
    "$$\n",
    "(\\mathcal{D}): \n",
    " \\left\\{\n",
    " \\begin{array}{lll}\n",
    " \\min_{\\mu} &\\frac{1}{2}\\mu^{\\top}GG^{t}\\mu-\\mu^{\\top}u\n",
    " \\\\\n",
    " \\mathrm{s.c.}& y^{\\top}\\mu = 0\n",
    " \\\\\n",
    " \\mathrm{and}& -\\mu \\leq  0\n",
    " \\end{array}\n",
    " \\right .\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    " G = \\begin{bmatrix}y_{1}x_{1}^{\\top} \\\\ \\vdots \\\\ y_{n}x_{n}^{\\top}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and $u = (1, \\dots, 1) \\in \\mathbb{R}^n$.\n",
    "\n",
    "We will **assume here qualification of the contraints**.\n",
    "\n",
    "Remark: The problem $(\\mathcal{D})$ is a *quadratic program* (QP) for which their exist off-the-shelf techniques. See below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A2: \n",
    "The Lagrangian is given by\n",
    "$$\n",
    "L(w,b,\\mu) = J(w,b)+\\mu \\cdot F(w,b)\n",
    "$$\n",
    "By Q1, we can apply KKT theorem for this problem and the dual problem reads\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\max_{\\mu} H(\\mu) \\\\\n",
    "\\mu \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "H(\\mu) = \\min_{w,b} L(w,b,\\mu)\n",
    "$$\n",
    "To get the explicit formula of $H$ we have\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\frac{\\partial}{\\partial w} L(w,b,\\mu) = 0 \\\\\n",
    "\\frac{\\partial}{\\partial b} L(w,b,\\mu) = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "from which we have\n",
    "$$\n",
    "\\begin{cases}\n",
    "w - G^\\top \\mu = 0 \\\\\n",
    "- \\mu \\cdot y = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "Then for $H$ we have\n",
    "$$\n",
    "H(\\mu) = \\frac{1}{2} \\mu^\\top G G^\\top \\mu + \\mu \\cdot (u - G G^\\top \\mu - y) = -\\frac{1}{2} \\mu^\\top G G^\\top \\mu + \\mu ^\\top u\n",
    "$$\n",
    "Thus the dual problem can be written as\n",
    "$$\n",
    "(\\mathcal{D}):\n",
    "\\begin{cases}\n",
    "\\min_{\\mu} \\frac{1}{2} \\mu^\\top G G^\\top \\mu - \\mu ^\\top u \\\\\n",
    "y^\\top \\mu = 0\\\\\n",
    "-\\mu \\leq 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q3: Justify that given the estimated $\\mu$, the prediction function for a new sample $x$ is given by:\n",
    "\n",
    "$$\n",
    "y = \\mathrm{sign}(\\sum_{i=1}^{n} \\mu_i y_i x_i^\\top x + b) \\enspace .\n",
    "$$\n",
    "\n",
    "The vector $w$ is therefore equal to $\\sum_{i=1}^{n} \\mu_i y_i x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A3:\n",
    "From Q2 we have\n",
    "$$w = G^\\top \\mu$$\n",
    "where $G^\\top = [y_1 x_1,...,y_n x_n]$. Thus\n",
    "$$ w = \\sum_{i=1}^n \\mu_i y_i x_i$$\n",
    "And the prediction function $f(x)=\\mathrm{sign}(w^{\\top}x + b)$ is then given by\n",
    "$$\n",
    "f(x) = \\mathrm{sign}(\\sum_{i=1}^{n} \\mu_i y_i x_i^\\top x + b) \\enspace .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implementation of solver with intercept using cvxopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file svm_project_utils.py contains the code to generate some toy data and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from svm_project_utils import plot_dataset, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAFkCAYAAAC9wjgoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d17ecf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, y = datasets(name='gaussian', n_points=300)\n",
    "plot_dataset(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The following function qp allows to solve a quadratic problem of the form:\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "  \\begin{array}{cll}\n",
    "  &\\min_{(x)} \\frac{1}{2}x^{\\top} H x - e^\\top x\n",
    "  \\\\\n",
    "   & \\textrm{s.c.}\\; A^\\top x = b, 0 \\leq x \\leq C.\n",
    "  \\end{array}\n",
    "  \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cvxopt\n",
    "\n",
    "\n",
    "def qp(H, e, A, b, C=np.inf, l=1e-8, verbose=True):\n",
    "    # Gram matrix\n",
    "    n = H.shape[0]\n",
    "    H = cvxopt.matrix(H)\n",
    "    A = cvxopt.matrix(y, (1, n))\n",
    "    e = cvxopt.matrix(-e)\n",
    "    b = cvxopt.matrix(0.0)\n",
    "    if C == np.inf:\n",
    "        G = cvxopt.matrix(np.diag(np.ones(n) * -1))\n",
    "        h = cvxopt.matrix(np.zeros(n))\n",
    "    else:\n",
    "        G = cvxopt.matrix(np.concatenate([np.diag(np.ones(n) * -1),\n",
    "                                         np.diag(np.ones(n))], axis=0))\n",
    "        h = cvxopt.matrix(np.concatenate([np.zeros(n), C * np.ones(n)]))\n",
    "\n",
    "    # Solve QP problem\n",
    "    cvxopt.solvers.options['show_progress'] = verbose\n",
    "    solution = cvxopt.solvers.qp(H, e, G, h, A, b)\n",
    " \n",
    "    # Lagrange multipliers\n",
    "    mu = np.ravel(solution['x'])\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the linear kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kernel(X1, X2):\n",
    "    n1 = X1.shape[0]\n",
    "    n2 = X2.shape[0]\n",
    "    K = np.empty((n1, n2))\n",
    "    for i in range(n1):\n",
    "        for j in range(n2):\n",
    "            K[i, j] = np.dot(X1[i], X2[j])\n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- Q4: Modify the following cell to solve the SVM dual problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = datasets(name='gaussian', n_points=300, sigma=0.7)\n",
    "plot_dataset(X, y)\n",
    "\n",
    "# TODO\n",
    "def svm_solver(K, y, C=np.inf):\n",
    "    H = None\n",
    "    e = None\n",
    "    A = None\n",
    "    b = None\n",
    "    mu = qp(H, e, A, b, C, l=1e-8, verbose=False)\n",
    "    idx_support = np.where(np.abs(mu) > 1e-5)[0]\n",
    "    mu_support = mu[idx_support]\n",
    "    return mu_support, idx_support\n",
    "\n",
    "K = kernel(X, X)\n",
    "\n",
    "# Uncomment the following lines when your svm_solver is completed:\n",
    "# mu_support, idx_support = svm_solver(K, y)\n",
    "# print(\"Number of support vectors: %s\" % idx_support.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q5: Compute w from mu and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "w = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q6: Using complementary slackness, explain how to obtain $b$ from $\\mu$.\n",
    "\n",
    "HINT: Use the fact that for all support vectors for which $\\mu_i$ is non-zero one has $y_{i}(w^{t}x_{i}+b) = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "def compute_b(K, y, mu_support, idx_support):\n",
    "    # TODO\n",
    "    y_support = y[idx_support]\n",
    "    K_support = K[idx_support][:, idx_support]\n",
    "    b = None\n",
    "    return b\n",
    "\n",
    "b = compute_b(K, y, mu_support, idx_support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q7: Verify that the constraints of the primal problem are satistified up to an acceptable numerical precision. You should verify that for all $i$ we have:\n",
    "\n",
    "$$\n",
    "y_{i}(w^{\\top}x_{i}+b) \\geq 1 - \\epsilon\n",
    "$$\n",
    "\n",
    "using for example $\\epsilon = 1e-5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your code by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = datasets(name='gaussian', n_points=300, sigma=0.7)\n",
    "\n",
    "K = kernel(X, X)\n",
    "mu_support, idx_support = svm_solver(K, y)\n",
    "b = compute_b(K, y, mu_support, idx_support)\n",
    "\n",
    "def plot_classif(X, y, mu_support, idx_support, b, kernel=kernel):\n",
    "    # Calcul de la fonction de décision sur une grille\n",
    "    X1, X2 = np.mgrid[-4:4:0.1, -4:4:0.1]\n",
    "    na, nb = X1.shape\n",
    "    X_test = np.c_[np.reshape(X1, (na * nb, 1)),\n",
    "                   np.reshape(X2, (na * nb, 1))]\n",
    "\n",
    "    # Calcul des produits scalaires\n",
    "    X_support = X[idx_support]\n",
    "    G = kernel(X_test, X_support)\n",
    "    # Calcul de la fonction de décision\n",
    "    decision = G.dot(mu_support * y[idx_support]) + b\n",
    "\n",
    "    # Calcul du label prédit\n",
    "    y_pred = np.sign(decision)\n",
    "\n",
    "    # Affichage des lignes de niveau de la fonction de decision\n",
    "    plt.contourf(X1, X2, np.reshape(decision, (na, nb)), 20, cmap=plt.cm.gray)\n",
    "    cs = plt.contour(X1, X2, np.reshape(decision, (na,nb)), [-1, 0, 1], color='g', linewidth=2)\n",
    "    plt.clabel(cs, inline=1)\n",
    "    plt.plot(X[y == 1,0], X[y == 1, 1], 'or', linewidth=2)\n",
    "    plt.plot(X[y == -1,0], X[y == -1, 1], 'ob', linewidth=2)\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlim([-4, 4])\n",
    "    plt.ylim([-4, 4])\n",
    "\n",
    "plot_classif(X, y, mu_support, idx_support, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now change the value of $\\sigma$ such that the problem is not linearily separable anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = datasets(name='gaussian', n_points=300, sigma=1.5)\n",
    "plot_dataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = kernel(X, X)\n",
    "mu_support, idx_support = svm_solver(K, y)\n",
    "b = compute_b(K, y, mu_support, idx_support)\n",
    "w = np.sum((mu_support * y[idx_support])[: , None] * X[idx_support], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q8: Check that contraints of the problem are now violated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Non separable case with cvxopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice it is very likely that the classes are not linearly separable.\n",
    "\n",
    "A very natural idea is to relax the constraints $y_{i}(w^\\top x_i + c) \\geq 1$.\n",
    "To do this, so called soft-margin SVM have been introduced using\n",
    "so called slack variables: $\\xi_{i}\\geq 0$. The problem becomes:\n",
    "\n",
    "$$\n",
    " y_{i}(w^\\top x_i + b) \\geq 1 - \\xi_i, \\; \\xi_i \\geq 0 \\enspace .\n",
    "$$\n",
    "\n",
    "Note that if $\\xi_i > 1$, the sample $x_{i}$ will be misclassified. To prevent\n",
    "this case to be too frequent, an idea is to minimize the sum of the $\\xi_{i}$.\n",
    "This leads to the following problem:\n",
    "\n",
    "$$\n",
    "(P_{s}):  \\left\\{\n",
    " \\begin{array}{ll}\n",
    " \\min_{(w,b,\\xi)} & \\frac{1}{2}w^{\\top}w + C \\sum_i \\xi_i\n",
    " \\\\\n",
    " \\mathrm{s.t.} & y_{i}(w^{\\top}x_{i}+b) \\geq 1 - \\xi_i\\\\\n",
    " \\mathrm{and} & -\\xi_i \\leq 0\n",
    " \\end{array}\n",
    " \\right.\n",
    "$$\n",
    "\n",
    "The constant $C$ controls the regularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q9: Justify that $(P_{s})$ is a convex problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A9: Again, the loss function $J(w,b,\\xi) = \\frac{1}{2} w^\\top w + C \\sum_i \\xi_i$ is quadratic and semi-definite positive and is thus convex. On the other hand, the constraint functions $1 - \\xi_i - y_i(w^\\top x_i + b)$ and $-\\xi$ are all affine and thus convex too. So, the problem $(P_s)$ is a convex problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Show that a dual problem of $(P_{s})$ reads:\n",
    "\n",
    "$$\n",
    "(\\mathcal{D}):\n",
    "\\left\\{\n",
    "\\begin{array}{lll}\n",
    "\\min_{\\mu} &\\frac{1}{2}\\mu^{\\top}GG^{\\top}\\mu-\\mu^{\\top}u\n",
    "\\\\\n",
    "\\mathrm{s.t.}& y^{\\top}\\mu = 0\n",
    "\\\\\n",
    "\\mathrm{et}& 0 \\leq \\mu \\leq C\n",
    "\\end{array}\n",
    "\\right .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \n",
    "The Lagrangian is given by\n",
    "$$\n",
    "L(w,b,\\xi,\\mu,\\nu) = \\frac{1}{2} w^\\top w + C\\sum_i \\xi_i + \\sum_i \\mu_i(1 - \\xi_i - y_i (w^\\top x_i +b)) + \\sum_i \\nu_i (-\\xi_i)\n",
    "$$\n",
    "and the dual problem reads\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\max_{\\mu, \\nu} H(\\mu,\\nu) \\\\\n",
    "\\mu \\geq 0 \\\\\n",
    "\\nu \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "H(\\mu, \\nu) = \\min_{w,b,\\xi} L(w,b,\\xi,\\mu,\\nu)\n",
    "$$\n",
    "To get the explicit formula of $H$ we have\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\frac{\\partial}{\\partial w} L(w,b,\\xi,\\mu,\\nu) = 0 \\\\\n",
    "\\frac{\\partial}{\\partial b} L(w,b,\\xi,\\mu,\\nu) = 0 \\\\\n",
    "\\frac{\\partial}{\\partial \\xi} L(w,b,\\xi,\\mu,\\nu) = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "from which we have\n",
    "$$\n",
    "\\begin{cases}\n",
    "w - G^\\top \\mu = 0 \\\\\n",
    "- \\mu \\cdot y = 0 \\\\\n",
    "C\\mathbb{1} - \\mu - \\nu = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "Then for $H$ we have\n",
    "$$\n",
    "H(\\mu) = \\frac{1}{2} \\mu^\\top G G^\\top \\mu + \\mu \\cdot (u - G G^\\top \\mu - y) = -\\frac{1}{2} \\mu^\\top G G^\\top \\mu + \\mu ^\\top u\n",
    "$$\n",
    "Thus the dual problem can be written as\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\min_{\\mu} \\frac{1}{2} \\mu^\\top G G^\\top \\mu - \\mu ^\\top u \\\\\n",
    "y^\\top \\mu = 0\\\\\n",
    "\\mu \\geq 0 \\\\\n",
    "\\nu \\geq 0 \\\\\n",
    "\\mu + \\nu = C \\mathbb{1}\n",
    "\\end{cases}\n",
    "$$\n",
    "which is equivalent to \n",
    "$$\n",
    "(\\mathcal{D}):\n",
    "\\begin{cases}\n",
    "\\min_{\\mu} \\frac{1}{2} \\mu^\\top G G^\\top \\mu - \\mu ^\\top u \\\\\n",
    "y^\\top \\mu = 0\\\\\n",
    "0 \\leq \\mu \\leq C \\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q10: Modify your code from Q4 to handle the non-separable case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "X, y = datasets(name='gaussian', n_points=300, sigma=1.7)\n",
    "\n",
    "K = kernel(X, X)\n",
    "# mu_support, idx_support = svm_solver(...)\n",
    "# b = compute_b(K, y, mu_support, idx_support)\n",
    "\n",
    "# plot_classif(X, y, mu_support, idx_support, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q11: What is the influence of C on the number of support vectors? Justify this from an optimization stand point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: non-linear case with kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another scenario is when the boundary between classes is not linear.\n",
    "\n",
    "To cope with this the idea is to use kernels.\n",
    "\n",
    "- Q12: Denoting by $K(x_i, x_j)$ the dot product between samples show that dual problem and the decision function f(x) can be reformulated just using calls to $K$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the clowns dataset to evaluate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = datasets(name='clowns', n_points=200, sigma=0.7)\n",
    "\n",
    "plot_dataset(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q13: Update your kernel function so it computes the Gaussian kernel:\n",
    "\n",
    "$$\n",
    "    K(x_i, x_j) = \\exp(-\\gamma \\| x_i - x_j \\|)\n",
    "$$\n",
    "\n",
    "where $\\gamma > 0$ is the kernel bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "from scipy import linalg\n",
    "\n",
    "def rbf_kernel(X1, X2):\n",
    "    n1 = X1.shape[0]\n",
    "    n2 = X2.shape[0]\n",
    "    K = np.empty((n1, n2))\n",
    "    gamma = 3.\n",
    "    for i in range(n1):\n",
    "        for j in range(n2):\n",
    "            K[i, j] = 0  # CHANGE THIS\n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the change above the follwing code should allow you to nicely separate the red from the blue dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = datasets(name='clowns', n_points=200, sigma=0.7)\n",
    "\n",
    "K = rbf_kernel(X, X)\n",
    "mu_support, idx_support = svm_solver(K, y, C=1.)\n",
    "b = compute_b(K, y, mu_support, idx_support)\n",
    "\n",
    "plot_classif(X, y, mu_support, idx_support, b, kernel=rbf_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Linear SVM without intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of the formuation of SVMs with the intercept term $b$ is that\n",
    "it leads to an annoying constraint in the dual, namely the $y^{t}\\mu = 0$.\n",
    "\n",
    "We will now see what we can do about it.\n",
    "\n",
    "Let's consider the problem\n",
    "\n",
    "$$\n",
    "(P'_{s}):  \\left\\{\n",
    " \\begin{array}{ll}\n",
    " \\min_{(w,\\xi)} & \\frac{1}{2}w^{\\top}w + C \\sum_i \\xi_i\n",
    " \\\\\n",
    " \\mathrm{s.t.} & y_{i}(w^{\\top}x_{i}) \\geq 1 - \\xi_i\\\\\n",
    " \\mathrm{and} & -\\xi_i \\leq 0\n",
    " \\end{array}\n",
    " \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q14: Show that a dual problem of $(P'_{s})$ is given by:\n",
    "\n",
    "$$\n",
    "(\\mathcal{D}):\n",
    "\\left\\{\n",
    "\\begin{array}{lll}\n",
    "\\min_{\\mu} &\\frac{1}{2}\\mu^{\\top}GG^{\\top}\\mu-\\mu^{\\top} 1_n\n",
    "\\\\\n",
    "\\mathrm{s.t.}& 0 \\leq \\mu \\leq C\n",
    "\\end{array}\n",
    "\\right .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q15: Rewrite the dual in the form:\n",
    "\n",
    "\n",
    "$$\n",
    "(\\mathcal{D}): \\min_{\\mu} f(\\mu) + g(\\mu) .\n",
    "$$\n",
    "\n",
    "where $f$ is here a smooth function of $\\mu$ with L-Liptschitz gradient and $g$ is a non-smooth function that is separable, namely:\n",
    "\n",
    "$$\n",
    "g(\\mu) = \\sum_{i=1}^n g_i(\\mu_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dual in the later form can be readily optimized using the methods that you have been studying in this class:\n",
    "\n",
    "- Proximal gradient method with and without acceleration\n",
    "- L-BFGS-B\n",
    "- Coordinate descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q16: Implement:\n",
    "\n",
    "   - your own version of proximal gradient with and without acceleration\n",
    "   - your own version of coordinate descent\n",
    "   - an L-BFGS-B solver using `scipy.optimize.fmin_l_bfgs_b`\n",
    "\n",
    "Note: We restrict ourselves to linear kernel here.\n",
    "\n",
    "Note: To handle separating hyperplanes which do not pass throw zero (due to abscence of intercept)\n",
    "you will add a column of ones to X. You can use something like this:\n",
    "\n",
    "`X = np.concatenate((X, np.ones((len(X), 1))), axis=1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will test your implementations on the Gaussian blobs and evaluate the performance of your implementations in terms of computation time on problems where the matrix $G G^\\top$ can fit in memory.\n",
    "\n",
    "You should reuse as much as possible the convergence evaluation code that you used during the labs.\n",
    "\n",
    "For a coordinate descent method to be fast you need to have smart updates. You're expected to\n",
    "come up with these smart updates in the problem at hand.\n",
    "\n",
    "BONUS : With a smart implementation of the coordinate descent you should be able to scale the optimization to tens of thousands of samples ie cases where $G G^\\top$ does not fit in memory anymore.\n",
    "\n",
    "**IMPORTANT : This question Q16 is the most important and will constitute half of the final grade on the project !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "from scipy.optimize import fmin_l_bfgs_b"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
